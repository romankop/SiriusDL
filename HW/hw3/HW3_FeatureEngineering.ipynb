{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Для датасета животных обучить MLP.\n",
    "2. Использовать Custom Dataset, Sampler, collate_fn\n",
    "3. Сделать различную предобработку фичей\n",
    "4. Подключить для логирования tensorboard и/или mlflow\n",
    "5. Не забыть разделить выборку на train и valid\n",
    "6. Получить точность не ниже 65%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_lightning.metrics import Accuracy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    # Конструктор, где считаем датасет\n",
    "    def __init__(self, X, target):\n",
    "\n",
    "        weekday_columns = ['Weekday_0', 'Weekday_1', 'Weekday_2',\n",
    "                           'Weekday_3', 'Weekday_4', 'Weekday_5', 'Weekday_6']\n",
    "        weekdays = np.argmax(X[weekday_columns].values, axis=1)\n",
    "\n",
    "        X.drop(weekday_columns, axis=1, inplace=True)\n",
    "\n",
    "        \n",
    "        X['Weekday_cos'] = np.cos(2 * np.pi / 7.) * weekdays\n",
    "        X['Weekday_sin'] = np.sin(2 * np.pi / 7.) * weekdays\n",
    "        X['Weekday_tan'] = np.tan(2 * np.pi / 7.) * weekdays\n",
    "\n",
    "        X['Hour_cos'] = np.cos(2 * np.pi / 24.) * X['Hour'].values\n",
    "        X['Hour_sin'] = np.sin(2 * np.pi / 24.) * X['Hour'].values\n",
    "        X['Hour_tan'] = np.tan(2 * np.pi / 24.) * X['Hour'].values\n",
    "\n",
    "        X['Month_cos'] = np.cos(2 * np.pi / 12.) * X['Month'].values\n",
    "        X['Month_sin'] = np.sin(2 * np.pi / 12.) * X['Month'].values\n",
    "        X['Month_tan'] = np.tan(2 * np.pi / 12.) * X['Month'].values\n",
    "\n",
    "        X['Gender'] = np.argmax(X[['Sex_Female', 'Sex_Male', 'Sex_Unknown']].values, axis=1)\n",
    "        X['SexStatus'] = np.argmax(X[['SexStatus_Flawed', 'SexStatus_Intact', 'SexStatus_Unknown']].values, axis=1)\n",
    "        X['Weekday'] = weekdays\n",
    "        X['Breed'] = np.argmax(X[['Breed_Chihuahua Shorthair Mix', 'Breed_Domestic Medium Hair Mix',\n",
    "                                'Breed_Domestic Shorthair Mix', 'Breed_German Shepherd Mix', 'Breed_Labrador Retriever Mix',\n",
    "                                 'Breed_Pit Bull Mix', 'Breed_Rare']].values, axis=1)\n",
    "        X['Hair'] = np.argmax(X[['Shorthair', 'Longhair']].values, axis=1)\n",
    "\n",
    "        X.drop(['Sex_Female', 'Sex_Male', 'Sex_Unknown', 'SexStatus_Flawed', 'SexStatus_Intact', 'SexStatus_Unknown',\n",
    "               'Breed_Chihuahua Shorthair Mix', 'Breed_Domestic Medium Hair Mix', 'Breed_Domestic Shorthair Mix',\n",
    "                'Breed_German Shepherd Mix', 'Breed_Labrador Retriever Mix', 'Breed_Pit Bull Mix', 'Shorthair', 'Longhair'], \n",
    "               axis=1, inplace=True)\n",
    "\n",
    "        target = target.iloc[:, :].values\n",
    "        target[target == 'Died'] = 'Euthanasia'\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        self.y = le.fit_transform(target)\n",
    "\n",
    "        self.X = X.values\n",
    "\n",
    "        self.columns = X.columns.values\n",
    "\n",
    "        self.embedding_columns = ['Gender', 'SexStatus', 'Weekday', 'Breed', 'Hair']\n",
    "        self.nrof_emb_categories = 22\n",
    "        self.numeric_columns = ['IsDog', 'Age', 'HasName', 'NameLength', 'NameFreq', 'MixColor', 'ColorFreqAsIs',\n",
    "                                'ColorFreqBase', 'TabbyColor', 'MixBreed', 'Domestic', \n",
    "                                'Year', 'Day', 'Weekday_cos', 'Weekday_sin', 'Weekday_tan',\n",
    "                                'Hour_cos', 'Hour_sin', 'Hour_tan', 'Month_cos', 'Month_sin', 'Month_tan']\n",
    "\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # Переопределяем метод,\n",
    "    # который достает по индексу наблюдение из датасет\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        row = self.X[idx, :]\n",
    "\n",
    "        row = {col: torch.tensor(row[i]) for i, col in enumerate(self.columns)}\n",
    "\n",
    "        return row, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSampler(Sampler):\n",
    "\n",
    "    # Конструктор, где инициализируем индексы элементов\n",
    "    def __init__(self, data):\n",
    "        self.data_indices = np.arange(len(data))\n",
    "\n",
    "        shuffled_indices = np.random.permutation(len(self.data_indices))\n",
    "\n",
    "        self.data_indices = np.ascontiguousarray(self.data_indices)[shuffled_indices]\n",
    "\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_indices)\n",
    "\n",
    "    # Возращает итератор,\n",
    "    # который будет возвращать индексы из перемешанного датасета\n",
    "    def __iter__(self):\n",
    "        return iter(self.data_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Gender': tensor(1., dtype=torch.float64), \n",
    "'SexStatus': tensor(0., dtype=torch.float64),\n",
    "'Weekday': tensor(5., dtype=torch.float64),\n",
    "'Breed': tensor(2., dtype=torch.float64),\n",
    "'Hair': tensor(0., dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    for i in range(len(batch)):\n",
    "        data = batch[i][0]\n",
    "        data['SexStatus'] += 3\n",
    "        data['Weekday'] += 6\n",
    "        data['Breed'] += 13\n",
    "        data['Hair'] += 20\n",
    "        batch[i] = (data, batch[i][1])\n",
    "    return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(train_dataset, valid_dataset, test_dataset, \n",
    "                       train_sampler, valid_sampler, test_sampler):\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_dataset, sampler=train_sampler,\n",
    "                              batch_size=BATCH_SIZE, collate_fn=collate,\n",
    "                              shuffle=False)\n",
    "    \n",
    "    valid_loader = DataLoader(dataset=valid_dataset, sampler=valid_sampler,\n",
    "                              batch_size=BATCH_SIZE, collate_fn=collate,\n",
    "                              shuffle=False)\n",
    "\n",
    "    test_loader = DataLoader(dataset=test_dataset, sampler=test_sampler,\n",
    "                             batch_size=BATCH_SIZE, collate_fn=collate,\n",
    "                             shuffle=False)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, nrof_cat, emb_dim,\n",
    "                 emb_columns, numeric_columns):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.emb_columns = emb_columns\n",
    "        self.numeric_columns = numeric_columns\n",
    "\n",
    "        self.emb_layer = torch.nn.Embedding(nrof_cat, emb_dim)\n",
    "\n",
    "        self.feature_bn = torch.nn.BatchNorm1d(input_size)\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.linear1.apply(self.init_weights)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.linear2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear2.apply(self.init_weights)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.linear3 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "            # m.bias.data.fill_(0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb_output = self.emb_layer(torch.tensor(pd.DataFrame(x)[self.emb_columns].values, dtype=torch.int64))\n",
    "        emb_output = emb_output.permute((0, 2, 1)).mean(-1)\n",
    "        numeric_feats = torch.tensor(pd.DataFrame(x)[self.numeric_columns].values, dtype=torch.float32)\n",
    "        \n",
    "        concat_input = torch.cat([numeric_feats, emb_output], dim=1)\n",
    "        output = self.feature_bn(concat_input)\n",
    "\n",
    "        output = self.linear1(output)\n",
    "        output = self.bn1(output)\n",
    "        output = torch.relu(output)\n",
    "\n",
    "        output = self.linear2(output)\n",
    "        output = self.bn2(output)\n",
    "        output = torch.relu(output)\n",
    "\n",
    "        output = self.linear3(output)\n",
    "        predictions = torch.softmax(output, dim=1)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = SummaryWriter('./logs/train')\n",
    "valid_writer = SummaryWriter('./logs/valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(HIDDEN_SIZE, OUTPUT_SIZE, LEARNING_RATE, EPOCHS,\n",
    "            BATCH_SIZE, EMBEDDING_SIZE):\n",
    "    ###\n",
    "    def train_inference():\n",
    "    \n",
    "        train_writer.add_text('LEARNING_RATE', str(LEARNING_RATE))\n",
    "        train_writer.add_text('INPUT_SIZE', str(INPUT_SIZE))\n",
    "        train_writer.add_text('HIDDEN_SIZE', str(HIDDEN_SIZE))\n",
    "        train_writer.add_text('OUTPUT_SIZE', str(OUTPUT_SIZE))\n",
    "        train_writer.add_text('EMBEDDING_SIZE', str(EMBEDDING_SIZE))\n",
    "        train_writer.add_text('BATCH_SIZE', str(BATCH_SIZE))\n",
    "        train_writer.add_text('EPOCHS', str(EPOCHS))\n",
    "        \n",
    "        valid_writer.add_text('LEARNING_RATE', str(LEARNING_RATE))\n",
    "        valid_writer.add_text('INPUT_SIZE', str(INPUT_SIZE))\n",
    "        valid_writer.add_text('HIDDEN_SIZE', str(HIDDEN_SIZE))\n",
    "        valid_writer.add_text('OUTPUT_SIZE', str(OUTPUT_SIZE))\n",
    "        valid_writer.add_text('EMBEDDING_SIZE', str(EMBEDDING_SIZE))\n",
    "        valid_writer.add_text('BATCH_SIZE', str(BATCH_SIZE))\n",
    "        valid_writer.add_text('EPOCHS', str(EPOCHS))\n",
    "        \n",
    "        step = 0\n",
    "        for epoch in tqdm(range(EPOCHS)):\n",
    "            model.train()\n",
    "\n",
    "            for features, label in train_loader:\n",
    "                # Reset gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                label = label.long()\n",
    "                output = model(features)\n",
    "                # Calculate error and backpropagate\n",
    "                loss = criterion(output, label)\n",
    "                loss.backward()\n",
    "                acc = accuracy(output, label).item()\n",
    "                \n",
    "                train_writer.add_scalar('CrossEntropyLoss', loss, step)\n",
    "                train_writer.add_scalar('Accuracy', acc, step)\n",
    "                train_writer.add_histogram('hidden_layer_1', model.linear1.weight.data, step)\n",
    "                train_writer.add_histogram('hidden_layer_2', model.linear2.weight.data, step)\n",
    "                train_writer.add_histogram('hidden_layer_3', model.linear3.weight.data, step)\n",
    "\n",
    "                # Update weights with gradients\n",
    "                optimizer.step()\n",
    "                \n",
    "            model.eval()\n",
    "\n",
    "            for features, label in valid_loader:\n",
    "                \n",
    "                label = label.long()\n",
    "                output = model(features)\n",
    "                # Calculate error and backpropagate\n",
    "                loss = criterion(output, label)\n",
    "                acc = accuracy(output, label).item()\n",
    "                \n",
    "                valid_writer.add_scalar('CrossEntropyLoss', loss, step)\n",
    "                valid_writer.add_scalar('Accuracy', acc, step)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            if step % 20 == 0:\n",
    "                print('EPOCH %d STEP %d : valid_loss: %f valid_acc: %f' %\n",
    "                      (epoch, step, loss.item(), acc))\n",
    "    \n",
    "    ###\n",
    "    def test_inference():\n",
    "        \n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for features, label in tqdm(test_loader):\n",
    "\n",
    "                output = model(features)\n",
    "                output = output.argmax(dim=1)\n",
    "                \n",
    "                total += len(label)\n",
    "                correct += (output == label).sum().item()\n",
    "                \n",
    "        return \"\\n Accuracy: \" + str(correct / total * 100)\n",
    "\n",
    "                    \n",
    "    ###\n",
    "    X = pd.read_csv(r\"C:\\Users\\Redmi\\Documents\\GitHub\\SiriusDL\\week06\\data\\X_cat.csv\", sep='\\t', index_col=0)\n",
    "    target = pd.read_csv(r\"C:\\Users\\Redmi\\Documents\\GitHub\\SiriusDL\\week06\\data\\y_cat.csv\", \n",
    "                         sep='\\t', index_col=0, names=['status'])  # header=-1,\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.values, target,\n",
    "                                                        test_size=0.2, stratify=target, random_state=42)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                        test_size=0.2, stratify=y_train, random_state=42)\n",
    "    \n",
    "    train_dataset = CustomDataset(pd.DataFrame(X_train, columns=X.columns), y_train)\n",
    "    valid_dataset = CustomDataset(pd.DataFrame(X_val, columns=X.columns), y_val)\n",
    "    test_dataset = CustomDataset(pd.DataFrame(X_test, columns=X.columns), y_test)\n",
    "    \n",
    "    train_sampler = CustomSampler(train_dataset.X)\n",
    "    valid_sampler = CustomSampler(valid_dataset.X)\n",
    "    test_sampler = CustomSampler(test_dataset.X)\n",
    "    \n",
    "    train_loader, valid_loader, test_loader = create_data_loader(train_dataset, valid_dataset, test_dataset,\n",
    "                                                                train_sampler, valid_sampler, test_sampler)\n",
    "    \n",
    "    INPUT_SIZE = EMBEDDING_SIZE + len(train_dataset.numeric_columns)\n",
    "\n",
    "    model = MLPNet(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, train_dataset.nrof_emb_categories,\n",
    "                   EMBEDDING_SIZE,\n",
    "                   train_dataset.embedding_columns, train_dataset.numeric_columns)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    accuracy = Accuracy()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "    train_inference()\n",
    "    return test_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_SIZE = 5\n",
    "LEARNING_RATE = 1e-2\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "EMBEDDING_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60e1814aa7a43d89f92f5218815d34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 19 STEP 20 : valid_loss: 1.210879 valid_acc: 0.701657\n",
      "EPOCH 39 STEP 40 : valid_loss: 1.227641 valid_acc: 0.668508\n",
      "EPOCH 59 STEP 60 : valid_loss: 1.204950 valid_acc: 0.696133\n",
      "EPOCH 79 STEP 80 : valid_loss: 1.184435 valid_acc: 0.718232\n",
      "EPOCH 99 STEP 100 : valid_loss: 1.182559 valid_acc: 0.723757\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb3f1c5d4dc47dd8500a2a3e0c3402a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=21.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n Accuracy: 65.86232697343809'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(HIDDEN_SIZE, OUTPUT_SIZE, LEARNING_RATE, EPOCHS, BATCH_SIZE, EMBEDDING_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
